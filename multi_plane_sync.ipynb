{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion + Multi-Plane Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, AutoencoderKL\n",
    "\n",
    "from models.multiplane_sync_legacy import apply_custom_processors_for_unet, apply_custom_processors_for_vae\n",
    "from utils.cube import images_to_equi_and_dice, concat_dice_mask\n",
    "\n",
    "\n",
    "def build_sd_pipeline(device, version: str):\n",
    "    if version == 'sd2':\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-2\",\n",
    "            torch_dtype=torch.float16,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "    \n",
    "    elif version == 'sdxl':\n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "            vae=AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16),\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            local_files_only=True,\n",
    "        )\n",
    "    \n",
    "    apply_custom_processors_for_unet(pipe.unet, enable_sync_self_attn=True, enable_sync_cross_attn=False, enable_sync_conv2d=True, enable_sync_gn=True)\n",
    "    apply_custom_processors_for_vae(pipe.vae, enable_sync_attn=True, enable_sync_gn=True, enable_sync_conv2d=True)\n",
    "    \n",
    "    return pipe.to(device)\n",
    "\n",
    "\n",
    "# Get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Build the Stable Diffusion pipeline with Multi-Plane Sync.\n",
    "version = 'sd2'  # 'sd2' or 'sdxl'\n",
    "pipe = build_sd_pipeline(device, version=version)\n",
    "\n",
    "# Image generation\n",
    "image_size = 512 if version == 'sd2' else 768\n",
    "prompts = [\"Vast cosmos in the style of Van Gogh\"] * 6\n",
    "latents = torch.randn(6, 4, image_size//8, image_size//8).to(device, dtype=torch.float16)\n",
    "images = pipe(prompts, latents=latents, output_type='np').images\n",
    "images = rearrange(images, '(b m) ... -> b m ...', m=6)\n",
    "\n",
    "# Visualization of generated images\n",
    "equis, dices = images_to_equi_and_dice(images)\n",
    "equi_rgb_pil = Image.fromarray((equis[0] * 255).astype(np.uint8))\n",
    "dice_rgb_pil = Image.fromarray((dices[0] * 255).astype(np.uint8))\n",
    "equi_rgb_pil.resize((1024, 512)).show()\n",
    "concat_dice_mask(dice_rgb_pil).resize((1024, 768)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marigold + Multi-Plane Synchronization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from typing import Optional, Union\n",
    "from diffusers.pipelines.marigold.pipeline_marigold_depth import MarigoldDepthPipeline\n",
    "\n",
    "from models.multiplane_sync_legacy import apply_custom_processors_for_unet, apply_custom_processors_for_vae\n",
    "from utils.cube import Cubemap, images_to_equi_and_dice, concat_dice_mask\n",
    "from utils.depth import z_distance_to_depth\n",
    "from utils.equi import Equirectangular\n",
    "\n",
    "\n",
    "def load_images_from_panorama(pano_path: str, cube_size: int) -> np.ndarray:\n",
    "    equi = np.array(Image.open(pano_path).convert('RGB'))\n",
    "    cube = Equirectangular(equi).to_cubemap(cube_size)\n",
    "    images = Cubemap.cube_all2all_equilib(cube.faces, cube.cube_format, 'list', to_equilib=True)\n",
    "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    images = rearrange(images, 'm h w c -> 1 m h w c', m=6)\n",
    "    return images\n",
    "\n",
    "\n",
    "def build_marigold_pipeline(device):\n",
    "    pipe = MarigoldDepthPipeline.from_pretrained(\n",
    "        'prs-eth/marigold-depth-v1-0',\n",
    "        variant=\"fp16\",\n",
    "        torch_dtype=torch.float16,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    apply_custom_processors_for_unet(pipe.unet, enable_sync_self_attn=True, enable_sync_cross_attn=True, enable_sync_conv2d=True, enable_sync_gn=True)\n",
    "    apply_custom_processors_for_vae(pipe.vae, enable_sync_attn=True, enable_sync_gn=True, enable_sync_conv2d=True)\n",
    "    return pipe.to(device)\n",
    "\n",
    "\n",
    "# Get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Build the Marigold pipeline with Multi-Plane Sync.\n",
    "pipe = build_marigold_pipeline(device)\n",
    "\n",
    "# Load cube images from a panorama\n",
    "images = load_images_from_panorama('assets/abandoned_hall.png', cube_size=512)\n",
    "\n",
    "# Depth estimation\n",
    "inputs = rearrange(torch.from_numpy(images), 'b m h w c -> (b m) c h w', m=6).to(device)\n",
    "depths = pipe(inputs, output_type='np', batch_size=6).prediction\n",
    "depths = rearrange(depths, '(b m) ... -> b m ...', m=6)\n",
    "depths = z_distance_to_depth(depths, 90.0, 90.0)\n",
    "\n",
    "# Visualization of images\n",
    "equis, dices = images_to_equi_and_dice(images)\n",
    "equi_rgb_pil = Image.fromarray((equis[0] * 255).astype(np.uint8))\n",
    "dice_rgb_pil = Image.fromarray((dices[0] * 255).astype(np.uint8))\n",
    "equi_rgb_pil.resize((1024, 512)).show()\n",
    "concat_dice_mask(dice_rgb_pil).resize((1024, 768)).show()\n",
    "\n",
    "# Visualization of depths\n",
    "val_min, val_max = np.percentile(depths, 2), np.percentile(depths, 98)  # 0.0, 1.0\n",
    "equis, dices = images_to_equi_and_dice(depths)\n",
    "equi_depth_vis = pipe.image_processor.visualize_depth(equis, val_min, val_max)[0]\n",
    "dice_depth_vis = pipe.image_processor.visualize_depth(dices, val_min, val_max)[0]\n",
    "equi_depth_vis.resize((1024, 512)).show()\n",
    "concat_dice_mask(dice_depth_vis).resize((1024, 768)).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
