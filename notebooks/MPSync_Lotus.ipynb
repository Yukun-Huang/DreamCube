{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "\n",
    "from external.Lotus.my_pipeline import build_lotus_pipeline\n",
    "from diffusers.pipelines.marigold.marigold_image_processing import MarigoldImageProcessor\n",
    "from models.multiplane_sync.processors_sd import apply_custom_processors_for_unet, apply_custom_processors_for_vae\n",
    "from utils.cube import Cubemap, images_to_equi_and_dice, concat_dice_mask\n",
    "from utils.depth import z_distance_to_depth\n",
    "from utils.equi import Equirectangular\n",
    "\n",
    "\n",
    "def load_images_from_panorama(pano_path: str, cube_size: int) -> np.ndarray:\n",
    "    equi = np.array(Image.open(pano_path).convert('RGB'))\n",
    "    cube = Equirectangular(equi).to_cubemap(cube_size)\n",
    "    images = Cubemap.cube_all2all_equilib(cube.faces, cube.cube_format, 'list', to_equilib=True)\n",
    "    images = np.array(images, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    images = rearrange(images, 'm h w c -> 1 m h w c', m=6)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Get device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Build the Marigold pipeline with Multi-Plane Sync.\n",
    "pipe = build_lotus_pipeline(\n",
    "    pretrained_model_name_or_path='jingheya/lotus-depth-g-v2-1-disparity',\n",
    "    mode='generation',\n",
    "    task_name='depth',\n",
    "    half_precision=True,\n",
    "    enable_xformers_memory_efficient_attention=False,\n",
    ").to(device)\n",
    "\n",
    "apply_custom_processors_for_unet(pipe.unet, enable_sync_self_attn=True, enable_sync_cross_attn=True, enable_sync_conv2d=True, enable_sync_gn=True)\n",
    "apply_custom_processors_for_vae(pipe.vae, mode='all', enable_sync_attn=True, enable_sync_gn=True, enable_sync_conv2d=True)\n",
    "\n",
    "# Load cube images from a panorama\n",
    "images = load_images_from_panorama('assets/abandoned_hall.png', cube_size=512)\n",
    "\n",
    "# Depth estimation\n",
    "inputs = rearrange(torch.from_numpy(images), 'b m h w c -> (b m) c h w', m=6).to(device)\n",
    "depths, _ = pipe.batch_inference(inputs)\n",
    "depths = depths[..., None]  # np.ndarray, (6, 512, 512, 1)\n",
    "depths = 1.0 - depths  # np.ndarray, (6, 512, 512, 1)\n",
    "depths = rearrange(depths, '(b m) ... -> b m ...', m=6)\n",
    "depths = z_distance_to_depth(depths, 90.0, 90.0)\n",
    "\n",
    "# Visualization of images\n",
    "equis, dices = images_to_equi_and_dice(images)\n",
    "equi_rgb_pil = Image.fromarray((equis[0] * 255).astype(np.uint8))\n",
    "dice_rgb_pil = Image.fromarray((dices[0] * 255).astype(np.uint8))\n",
    "equi_rgb_pil.resize((1024, 512)).show()\n",
    "concat_dice_mask(dice_rgb_pil).resize((1024, 768)).show()\n",
    "\n",
    "# Visualization of depths\n",
    "val_min, val_max = np.percentile(depths, 2), np.percentile(depths, 98)  # 0.0, 1.0\n",
    "equis, dices = images_to_equi_and_dice(depths)\n",
    "equi_depth_vis = MarigoldImageProcessor.visualize_depth(equis, val_min, val_max)[0]\n",
    "dice_depth_vis = MarigoldImageProcessor.visualize_depth(dices, val_min, val_max)[0]\n",
    "equi_depth_vis.resize((1024, 512)).show()\n",
    "concat_dice_mask(dice_depth_vis).resize((1024, 768)).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
