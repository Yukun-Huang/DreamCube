{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "from diffusers.utils import export_to_video\n",
    "from external.video_crafter_diffusers.pipeline_text_to_video_videocrafter import TextToVideoVideoCrafterPipeline, tensor2vid\n",
    "from external.video_crafter_diffusers.unet_3d_videocrafter import UNet3DVideoCrafterConditionModel\n",
    "from models.multiplane_sync.processors_videocrafter2 import (\n",
    "    apply_custom_processors_for_unet,\n",
    "    apply_custom_processors_for_vae,\n",
    ")\n",
    "\n",
    "pipe = TextToVideoVideoCrafterPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\n",
    "pipe.unet = UNet3DVideoCrafterConditionModel.from_pretrained(\"adamdad/videocrafterv2_diffusers\", torch_dtype=torch.float16)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras=True, algorithm_type=\"sde-dpmsolver++\")\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# pipe.vae.enable_tiling()\n",
    "# pipe.vae.enable_slicing()\n",
    "\n",
    "apply_custom_processors_for_unet(\n",
    "    pipe.unet,\n",
    "    enable_sync_self_attn=True,\n",
    "    enable_sync_cross_attn=False,\n",
    "    enable_sync_conv2d=True,\n",
    "    enable_sync_gn=True,\n",
    ")\n",
    "apply_custom_processors_for_vae(\n",
    "    pipe.vae,\n",
    "    mode='all',\n",
    "    enable_sync_attn=False,\n",
    "    enable_sync_conv2d=True,\n",
    "    enable_sync_gn=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.amp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange, repeat\n",
    "from utils.cube import images_to_equi_and_dice\n",
    "\n",
    "\n",
    "def frames_to_pano_video(frames):\n",
    "    assert frames.ndim == 5 and frames.shape[0] % 6 == 0, f\"Expected 5D tensor with shape (B*M, F, H, W, C), got {frames.shape}\" \n",
    "    \n",
    "    frames = rearrange(frames, \"(B M) F H W C -> B M F H W C\", M=6)\n",
    "\n",
    "    B, M, F, H, W, C = frames.shape\n",
    "\n",
    "    pano_frames, cube_frames = [], []\n",
    "    for i in range(F):\n",
    "        panos, cubes = images_to_equi_and_dice(frames[:, :, i])\n",
    "        pano_frames.append(panos)  # [B, H, W, C]\n",
    "        cube_frames.append(cubes)  # [B, H, W, C]\n",
    "    pano_frames = np.stack(pano_frames, axis=1)  # [B, F, H, W, C]\n",
    "    cube_frames = np.stack(cube_frames, axis=1)  # [B, F, H, W, C]\n",
    "\n",
    "    return pano_frames, cube_frames\n",
    "\n",
    "\n",
    "def decode_latents(vae, latents):\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "\n",
    "    batch_size, channels, num_frames, height, width = latents.shape\n",
    "    # latents = latents.permute(0, 2, 1, 3, 4).reshape(batch_size * num_frames, channels, height, width)\n",
    "    latents = latents.permute(0, 2, 1, 3, 4)  # [batch_size, num_frames, channels, height, width]\n",
    "\n",
    "    image = [vae.decode(latents[:, i]).sample for i in range(num_frames)]\n",
    "    image = torch.stack(image, dim=1)  # [batch_size, num_frames, channels, height, width]\n",
    "    image = rearrange(image, 'b f c h w -> (b f) c h w')\n",
    "\n",
    "    video = image[None, :].reshape((batch_size, num_frames, -1) + image.shape[2:]).permute(0, 2, 1, 3, 4)\n",
    "    # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
    "    video = video.float()\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "prompt = 'Underwater world, Ghibli style'\n",
    "filename = 'vc2'\n",
    "height = width = 512\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        pipe.unet = pipe.unet.to('cuda')\n",
    "        pipe.text_encoder = pipe.text_encoder.to('cuda')\n",
    "        \n",
    "        video_frames = pipe(\n",
    "            [prompt] * 6,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=16, # donot modify this because it is used in the multiplane sync processor\n",
    "            num_inference_steps=50,\n",
    "            output_type='latent',\n",
    "        ).frames  # torch.Tensor, (6, 4, 16, 64, 64)\n",
    "\n",
    "        pipe.unet = pipe.unet.to('cpu')\n",
    "        pipe.text_encoder = pipe.text_encoder.to('cpu')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        video_frames = decode_latents(pipe.vae, video_frames)\n",
    "        video_frames = tensor2vid(video_frames, pipe.image_processor, output_type='np')\n",
    "\n",
    "\n",
    "pano_frames, cube_frames = frames_to_pano_video(video_frames)  # (6, 16, 1024, 2048, 3), ...\n",
    "\n",
    "Image.fromarray(np.asarray(pano_frames[0][0] * 255.0, np.uint8)).save(f'{filename}_pano_frame.jpg')\n",
    "Image.fromarray(np.asarray(cube_frames[0][0] * 255.0, np.uint8)).save(f'{filename}_cube_frame.jpg')\n",
    "\n",
    "export_to_video(pano_frames[0], output_video_path=f'{filename}_pano.mp4', fps=8)\n",
    "export_to_video(cube_frames[0], output_video_path=f'{filename}_cube.mp4', fps=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
